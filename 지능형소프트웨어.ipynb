{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfNR8c3cPpTUD4RBpb2Fvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeonyuB/Uchan/blob/Hell/%EC%A7%80%EB%8A%A5%ED%98%95%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rcTAt-M7w8cZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import music21\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "states = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "P = np.array([\n",
        "  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "  [0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.4, 0.0],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "  [0.0, 0.1, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0],\n",
        "  [0.0, 0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0],\n",
        "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "])\n",
        "\n",
        "R = np.array([0, -2, -2, -2, 10, -1, 1, 0])\n",
        "\n",
        "V = np.zeros(len(states))\n",
        "threshold = 0.0001\n",
        "\n",
        "def value_iteration (gamma):\n",
        "\n",
        "  while True:\n",
        "    V_new = np.copy(V)\n",
        "\n",
        "    for s in states:\n",
        "      V_new[s] = R[s] + gamma * np.sum([P[s, s_prime] * V[s_prime] for s_prime in states])\n",
        "\n",
        "    if np.max(np.abs(V_new - V)) < threshold:\n",
        "      V = V_new\n",
        "      break\n",
        "\n",
        "    V = V_new\n",
        "  print(\"Value function:\", V)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4x4 small gridworld"
      ],
      "metadata": {
        "id": "yAZO6LNky9VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (4, 4)\n",
        "terminals = [(0, 0), (3, 3)]\n",
        "\n",
        "numa = 4\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "reward = -1 * np.ones(shape)\n",
        "for terminal in terminals:\n",
        "    reward[terminal] = 0\n",
        "\n",
        "gamma = 1.0\n",
        "\n",
        "def P(state, action):\n",
        "    if action == 'up':\n",
        "        next_state = (max(0, state[0]-1), state[1])\n",
        "    elif action == 'down':\n",
        "        next_state = (min(shape[0]-1, state[0]+1), state[1])\n",
        "    elif action == 'left':\n",
        "        next_state = (state[0], max(0, state[1]-1))\n",
        "    elif action == 'right':\n",
        "        next_state = (state[0], min(shape[1]-1, state[1]+1))\n",
        "    return next_state\n",
        "\n",
        "V = np.zeros(shape)\n",
        "\n",
        "while True:\n",
        "    delta = 0\n",
        "    for i in range(shape[0]):\n",
        "        for j in range(shape[1]):\n",
        "            if (i, j) in terminals:\n",
        "                continue\n",
        "            v = V[i, j]\n",
        "            V[i,j]= sum((reward[i, j] + gamma * V[P((i, j), a)]) for a in actions)/numa\n",
        "            delta = max(delta, abs(v - V[i, j]))\n",
        "    if delta < 1e-4:\n",
        "        break\n",
        "\n",
        "optimal_policy = {}\n",
        "for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "        if (i, j) in terminals:\n",
        "            optimal_policy[i, j] = 'terminal'\n",
        "        else:\n",
        "            optimal_policy[i, j] = actions[np.argmax([(reward[i,j]\n",
        "                                                       +gamma*V[P((i,j),a)]) for a in actions])]\n",
        "\n",
        "print(\"Optimal policy is:\")\n",
        "for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "        print(f\"({i},{j}): {optimal_policy[i, j]}\")\n",
        "\n",
        "print(\"\\nOptimal value function is:\")\n",
        "for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "        print(f\"V({i},{j}): {V[i, j]}\")\n"
      ],
      "metadata": {
        "id": "d7Cwpi-1zC7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2d6593-15d0-4bf2-ebb3-5b1d5f3b583a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy is:\n",
            "(0,0): terminal\n",
            "(0,1): left\n",
            "(0,2): left\n",
            "(0,3): left\n",
            "(1,0): up\n",
            "(1,1): up\n",
            "(1,2): left\n",
            "(1,3): down\n",
            "(2,0): up\n",
            "(2,1): up\n",
            "(2,2): right\n",
            "(2,3): down\n",
            "(3,0): up\n",
            "(3,1): right\n",
            "(3,2): right\n",
            "(3,3): terminal\n",
            "\n",
            "Optimal value function is:\n",
            "V(0,0): 0.0\n",
            "V(0,1): -13.999312424461952\n",
            "V(0,2): -19.999011518162753\n",
            "V(0,3): -21.998911992496346\n",
            "V(1,0): -13.999312424461952\n",
            "V(1,1): -17.999156254598965\n",
            "V(1,2): -19.99908388638086\n",
            "V(1,3): -19.99909436158647\n",
            "V(2,0): -19.999011518162757\n",
            "V(2,1): -19.99908388638086\n",
            "V(2,2): -17.99922696784339\n",
            "V(2,3): -13.999422844683943\n",
            "V(3,0): -21.99891199249635\n",
            "V(3,1): -19.999094361586472\n",
            "V(3,2): -13.999422844683945\n",
            "V(3,3): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy iteration 구현"
      ],
      "metadata": {
        "id": "9tNCcZiRk5LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy={state: np.random.choice(actions) for state in\n",
        "        [(i,j) for i in range(shape[0]) for j in range(shape[1])] if state not in terminals}\n",
        "\n",
        "while True:\n",
        "  while True:\n",
        "    delta=0\n",
        "    for i in range(shape[0]):\n",
        "      for j in range(shape[1]):\n",
        "        if (i,j) in terminals:\n",
        "          continue\n",
        "        v=V[i,j]\n",
        "        action=policy[(i,j)]\n",
        "        V[i,j]=reward[i,j]+gamma*V[P((i,j),action)]\n",
        "        delta=max(delta,abs(v-V[i,j]))\n",
        "    if delta<2.01:\n",
        "      break\n",
        "  policy_stable=True\n",
        "  for j in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "        if (i,j) in terminals:\n",
        "          continue\n",
        "        old_action=policy[(i,j)]\n",
        "        policy[(i,j)]=actions[np.argmax([reward[i,j]\n",
        "                                         +gamma*V[P((i,j),action)] for action in actions])]\n",
        "        if old_action!=policy[(i,j)]:\n",
        "          policy_stable=False\n",
        "  if policy_stable:\n",
        "    break"
      ],
      "metadata": {
        "id": "MlYRnvCdvXaV"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}